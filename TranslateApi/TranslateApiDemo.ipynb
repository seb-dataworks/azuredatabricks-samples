{"cells":[{"cell_type":"markdown","source":["## Translate text on Azure Data Lake Store using Azure Databricks and Azure Translate API\n\nThis example reads text from a CSV file on a Azure Data Lake Store (ADLS), translates it into English and writes the result back to ADLS.\nThe text can be in various languages, the Translate API will automatically infer the source language.\n\nRequires:\n - Python 3\n - An Azure AD service principle with access to the ADLS\n - A Translate service provisioned in Azure and its subscription key"],"metadata":{}},{"cell_type":"code","source":["### Variables ###\n\n## TODO: Fill in those values ##########################\n\ntranslationApiKey = \"*******************\"\n\naadServicePrinciple_clientId = \"***-******************-*******-**-***\"\naadServicePrinciple_key = \"***************\"\naadTenantId = \"**-****-****-****-****\"\n\nadlsAccountName = \"MYADLSNAME\"\ninputFilePath = \"/cognitiveservicesdemo/translation_input.csv\"\noutputPath = \"/cognitiveservicesdemo/translation_result\""],"metadata":{},"outputs":[],"execution_count":2},{"cell_type":"code","source":["# Set ADLS credentials\n# See here for details how to configure: https://docs.databricks.com/spark/latest/data-sources/azure/azure-datalake.html#id3\n\nspark.conf.set(\"dfs.adls.oauth2.access.token.provider.type\", \"ClientCredential\")\nspark.conf.set(\"dfs.adls.oauth2.client.id\", aadServicePrinciple_clientId)\nspark.conf.set(\"dfs.adls.oauth2.credential\", aadServicePrinciple_key)\nspark.conf.set(\"dfs.adls.oauth2.refresh.url\", \"https://login.microsoftonline.com/\" + aadTenantId + \"/oauth2/token\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["# Example source: https://docs.microsoft.com/en-us/azure/cognitive-services/translator/quickstart-python-translate#translate-request\n\n###### Add your API key below ####\n\n# -*- coding: utf-8 -*-\n\nimport http.client, urllib.parse, uuid, json\n\nsubscriptionKey = translationApiKey\n\nhost = 'api.cognitive.microsofttranslator.com'\npath = '/translate?api-version=3.0'\n\n# Translate to English\nparams = \"&to=en\";\n\ndef translate (inputText):\n\n    requestBody = [{\n      'Text' : inputText,\n    }]\n    \n    content = json.dumps(requestBody, ensure_ascii=False).encode('utf-8')\n  \n    headers = {\n        'Ocp-Apim-Subscription-Key': subscriptionKey,\n        'Content-type': 'application/json',\n        'X-ClientTraceId': str(uuid.uuid4())\n    }\n\n    conn = http.client.HTTPSConnection(host)\n    conn.request (\"POST\", path + params, content, headers)\n    response = conn.getresponse ()\n    result = response.read ()\n    jsonOutput = json.loads(result.decode(\"utf-8\"))\n    return jsonOutput"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["def translateText(inputText):\n    translationResult = translate(inputText)\n    # Since we only translate to English, we always just get the first translation result. TODO: Add some error/null handling here\n    translatedText = translationResult[0]['translations'][0]['text']\n    return translatedText"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# Define translateText function as UDF so we can use it below directly on the DF\nfrom pyspark.sql.functions import udf\ntranslateTextUdf=udf(translateText)"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["# Read CSV from ADLS\ninputDF = sqlContext \\\n            .read \\\n            .format(\"csv\") \\\n            .load(\"adl://\" + adlsAccountName +\".azuredatalakestore.net\" + inputFilePath, header='true', inferSchema='true')"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["# Translate 'text' column in each row of the dataframe and write it back into a new column 'translated'\ntranslatedDF = inputDF.withColumn(\"translatedEN\", translateTextUdf(\"text\"))"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["display(translatedDF)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["# Write translated dataframe back as CSV to the ADLS\n# Note: this writed back a partitioned CSV file (or multiple). If you want to merge them, you can use .coalesce(1) but beware of possible performance implications for big data sets\ntranslatedDF \\\n  .write.option(\"header\", \"true\") \\\n  .option(\"quoteAll\", \"true\") \\\n  .option(\"encoding\", \"UTF-8\") \\\n  .mode(\"Overwrite\") \\\n  .csv(\"adl://\" + adlsAccountName +\".azuredatalakestore.net\" + outputPath + \".tmp/\")"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["## Optional ##\n## If the write() above only produced one .csv file (only one partition, or coalesce(1) was used), you can use these to copy and rename the resulting csv file\ncsv = list(filter(lambda file: file.path.endswith(\".csv\"), dbutils.fs.ls(\"adl://\" + adlsAccountName +\".azuredatalakestore.net\" + outputPath + \".tmp/\")))\ndbutils.fs.cp(csv[0].path, \"adl://\" + adlsAccountName +\".azuredatalakestore.net\" + outputPath + \".csv\")\ndbutils.fs.rm(\"adl://\" + adlsAccountName +\".azuredatalakestore.net\" + outputPath + \".tmp\", recurse= True)"],"metadata":{},"outputs":[],"execution_count":11}],"metadata":{"name":"TranslateApiDemo-clean","notebookId":4129707609834723},"nbformat":4,"nbformat_minor":0}
